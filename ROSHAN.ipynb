{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871e332e-4320-4a59-8dad-f7a487772b84",
   "metadata": {},
   "source": [
    "# ðŸ”¥ **ROSHAN Interactive Usage Notebook**\n",
    "### *A Simple Guide to Configuring and Running ROSHAN*\n",
    "\n",
    "**ROSHAN** is a simulation and reinforcement-learning framework for autonomous wildfire suppression using UAV agents.  \n",
    "It combines a high-fidelity C++ wildfire simulator with a Python-based hierarchical learning architecture. Although the internal system is complex, using ROSHAN is straightforward: nearly all behavior is controlled through a single YAML configuration file that specifies what to train, how to train it, and under which conditions simulations are executed.\n",
    "\n",
    "This notebook provides a lightweight interface for working with ROSHAN without needing to modify any internal code. Its purpose is to help users understand the configuration file, adjust selected parameters, and execute training or evaluation runs from a unified environment. It offers a practical, accessible entry point into ROSHAN without requiring any knowledge of its underlying implementation details.\n",
    "\n",
    "The workflow in this notebook is simple:\n",
    "\n",
    "1. **Load the main ROSHAN configuration file.**  \n",
    "   This file governs almost every aspect of the system.\n",
    "\n",
    "2. **Explain the core components of the configuration.**  \n",
    "   These include:\n",
    "   - **Hierarchy level** to train or evaluate (FlyAgent, ExploreAgent, PlannerAgent)  \n",
    "   - **Algorithmic hyperparameters** (learning rate, discount factors, PPO settings)  \n",
    "   - **Agent hyperparameters** (movement limits, observation configuration, rewards, etc.)  \n",
    "   - **General settings** (experiment seeds, environment options, logging paths, etc.)\n",
    "\n",
    "3. **Allow the user to modify selected configuration values.**  \n",
    "   Only parameters that are meaningful and safe to change from the notebook are exposed.  \n",
    "   More specialized aspectsâ€”like internal network architecturesâ€”must be changed in the source code, and ROSHAN automatically records architecture changes alongside the model weights.\n",
    "\n",
    "4. **Generate an execution-ready config file** and **run the simulation or training** directly from the notebook.  \n",
    "   The C++ simulation is invoked from Python, fully abstracted away, ensuring the system behaves consistently with the provided configuration.\n",
    "\n",
    "\n",
    "### ðŸ“ Load the Base ROSHAN Configuration\n",
    "\n",
    "Before we modify or execute anything, we first load the main `notebook_config.yaml` file.  \n",
    "This file contains all parameters used by ROSHAN during training or evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7a3434-d2b5-400c-8166-b21bd8bb1746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': {'IQL': {'batch_size': 250,\n",
      "                       'beta1': 0.9,\n",
      "                       'beta2': 0.999,\n",
      "                       'buffer_path': 'models/PPOCollisionBaseline_OptimizedReward_Top10Mean_Tanh/training_8/memory.pkl',\n",
      "                       'discount': 0.99,\n",
      "                       'expectile': 0.7,\n",
      "                       'k_epochs': 1,\n",
      "                       'lr': 0.0001575,\n",
      "                       'offline_updates': 10,\n",
      "                       'online_updates': 0,\n",
      "                       'policy_freq': 50,\n",
      "                       'tau': 0.005,\n",
      "                       'temperature': 3.0},\n",
      "               'PPO': {'_lambda': 0.98,\n",
      "                       'batch_size': 4000,\n",
      "                       'beta1': 0.9,\n",
      "                       'beta2': 0.999,\n",
      "                       'decay_rate': 0.9999,\n",
      "                       'entropy_coeff': 0.0005,\n",
      "                       'eps_clip': 0.2,\n",
      "                       'gamma': 0.9,\n",
      "                       'horizon': 120000,\n",
      "                       'k_epochs': 3,\n",
      "                       'kl_early_stop': False,\n",
      "                       'kl_target': 0.2,\n",
      "                       'lr': 0.0001575,\n",
      "                       'manual_decay': False,\n",
      "                       'memory_size': 1000000.0,\n",
      "                       'separate_optimizers': False,\n",
      "                       'use_kl_1': True,\n",
      "                       'use_logstep_decay': False,\n",
      "                       'value_loss_coef': 0.5},\n",
      "               'TD3': {'batch_size': 512,\n",
      "                       'beta1': 0.9,\n",
      "                       'beta2': 0.999,\n",
      "                       'discount': 0.99,\n",
      "                       'exploration_noise': 0.15,\n",
      "                       'k_epochs': 1,\n",
      "                       'lr': 0.0001,\n",
      "                       'memory_size': 1000000.0,\n",
      "                       'min_memory_size': 512,\n",
      "                       'noise_clip': 0.5,\n",
      "                       'policy_freq': 2,\n",
      "                       'policy_noise': 0.2,\n",
      "                       'tau': 0.005},\n",
      "               'share_encoder': True,\n",
      "               'use_tanh_dist': True},\n",
      " 'environment': {'agent': {'drone_size': 2,\n",
      "                           'explore_agent': {'algorithm': 'no_algo',\n",
      "                                             'default_model_folder': 'models/SimpleFlyAgent/',\n",
      "                                             'default_model_name': 'best_reward',\n",
      "                                             'frame_skips': 1,\n",
      "                                             'max_speed': 20.0,\n",
      "                                             'num_agents': 12,\n",
      "                                             'time_steps': 3,\n",
      "                                             'view_range': 20},\n",
      "                           'fly_agent': {'algorithm': 'PPO',\n",
      "                                         'collision': True,\n",
      "                                         'frame_skips': 1,\n",
      "                                         'max_speed': 10.0,\n",
      "                                         'num_agents': 4,\n",
      "                                         'rewards': {'BoundaryTerminal': -4.328008565212994,\n",
      "                                                     'Collision': -4.328008565212994,\n",
      "                                                     'DistanceImprovement': 0.29559098335108996,\n",
      "                                                     'Extinguish': 0.007235210886951731,\n",
      "                                                     'GoalReached': 0.8785929835412102,\n",
      "                                                     'ProximityPenalty': -0.11500008188778862,\n",
      "                                                     'TimeOut': -3.732236611159567},\n",
      "                                         'time_steps': 3,\n",
      "                                         'use_simple_policy': False,\n",
      "                                         'use_vel_bins': True,\n",
      "                                         'use_velocity_change': False,\n",
      "                                         'view_range': 10},\n",
      "                           'planner_agent': {'algorithm': 'PPO',\n",
      "                                             'default_model_folder': 'models/SimpleFlyAgent/',\n",
      "                                             'default_model_name': 'best_reward',\n",
      "                                             'frame_skips': 1,\n",
      "                                             'hierarchy_timesteps': 3,\n",
      "                                             'max_speed': 10.0,\n",
      "                                             'num_agents': 4,\n",
      "                                             'rewards': {'ExtinguishFires': 0.6054212968617818,\n",
      "                                                         'FastExtinguish': 0.21654104693212245,\n",
      "                                                         'FlyingTowardsGroundStation': -0.29328248592328265,\n",
      "                                                         'GoalReached': 3.9105863581302542,\n",
      "                                                         'MapBurnedTooMuch': -2.1999479401238373,\n",
      "                                                         'SameGoalPenalty': -1.552461776799054,\n",
      "                                                         'TimeOut': -2.3843039593546047},\n",
      "                                             'time_steps': 3,\n",
      "                                             'view_range': 10},\n",
      "                           'recharge_time': 0.0,\n",
      "                           'use_water_limit': False,\n",
      "                           'water_capacity': 5},\n",
      "                 'agent_behaviour': {'fire_goal_percentage': 1,\n",
      "                                     'groundstation_start_percentage': 1},\n",
      "                 'fire_behaviour': {'fire_noise': 1,\n",
      "                                    'fire_percentage': 0.01,\n",
      "                                    'fire_spread_prob': 0.5,\n",
      "                                    'num_fire_clusters': 2}},\n",
      " 'fire_model': {'rendering': {'background_color': [21, 19, 51, 255],\n",
      "                              'has_noise': True,\n",
      "                              'lingering': False,\n",
      "                              'noise_level': 20,\n",
      "                              'noise_size': 2,\n",
      "                              'render_grid': False},\n",
      "                'simulation': {'cells': {'cell_burning_duration': 120.0,\n",
      "                                         'cell_ignition_threshold': 100.0,\n",
      "                                         'cell_size': 10.0,\n",
      "                                         'flood_duration': 5.0},\n",
      "                               'grid': {'exploration_map_size': 50,\n",
      "                                        'uniform_nx': 50,\n",
      "                                        'uniform_ny': 50},\n",
      "                               'particles': {'Lt': 80.0,\n",
      "                                             'emit_convective': True,\n",
      "                                             'emit_radiation': True,\n",
      "                                             'radiationparticle_y_lim': 0.165,\n",
      "                                             'radiationparticle_y_st': 1.0,\n",
      "                                             'virtualparticle_c0': 1.98,\n",
      "                                             'virtualparticle_fl': 0.15,\n",
      "                                             'virtualparticle_tau_mem': 10.0,\n",
      "                                             'virtualparticle_y_lim': 0.2,\n",
      "                                             'virtualparticle_y_st': 1.0},\n",
      "                               'time': {'dt': 0.1,\n",
      "                                        'max_dt': 5.0,\n",
      "                                        'min_dt': 0.0001},\n",
      "                               'wind': {'wind_a': 0.314,\n",
      "                                        'wind_angle': -1.0,\n",
      "                                        'wind_uw': 10.0}}},\n",
      " 'paths': {'corine_dataset_name': 'CLMS_CLCplus_RASTER_2021_010m_eu_03035_V1_1.tif',\n",
      "           'init_map': '35x36_Woods.tif',\n",
      "           'model_directory': 'models/NotebookTest',\n",
      "           'model_name': ''},\n",
      " 'settings': {'auto_train': {'max_eval': 100,\n",
      "                             'max_train': 100,\n",
      "                             'train_episodes': 3,\n",
      "                             'use_auto_train': False},\n",
      "              'cia_mode': False,\n",
      "              'eval_fly_policy': False,\n",
      "              'hierarchy_type': 'fly_agent',\n",
      "              'llm_support': False,\n",
      "              'log_eval': True,\n",
      "              'mode': 2,\n",
      "              'optuna': {'n_startup_trials': 15,\n",
      "                         'n_trials': 200,\n",
      "                         'n_warmup_steps': 200,\n",
      "                         'objective': 'objective',\n",
      "                         'study_name': 'fly_model_study_LR',\n",
      "                         'study_root': 'models',\n",
      "                         'use_optuna': False,\n",
      "                         'use_pruning': True},\n",
      "              'pytorch_detect_anomaly': False,\n",
      "              'resume': False,\n",
      "              'rl_mode': 'train',\n",
      "              'save_replay_buffer': False,\n",
      "              'save_size': 200000.0,\n",
      "              'seed': -1,\n",
      "              'skip_gui_init': True}}\n"
     ]
    }
   ],
   "source": [
    "import yaml, os\n",
    "from pprint import pprint\n",
    "from src.pysim.utils import get_project_paths\n",
    "\n",
    "root_path = get_project_paths(\"root_path\")\n",
    "config_path = os.path.join(root_path, 'notebook_config.yaml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20812b69-5bce-4888-8f5e-b733e4c328ed",
   "metadata": {},
   "source": [
    "### ðŸ“˜ Understanding the ROSHAN Configuration Dictionary\n",
    "\n",
    "Now that you have inspected the full configuration dictionary, this section provides a structured explanation of its core components.  \n",
    "The goal is not to describe every value, but to give a clear understanding of how ROSHAN interprets the configuration and which parts matter for training or evaluationâ€”especially when using **PPO** and working with the **FlyAgent** or **PlannerAgent** hierarchies.\n",
    "\n",
    "The configuration is divided into several top-level sections:\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ **1. `settings` â€” Global Behavior of the Framework**\n",
    "This section defines the overall execution mode of ROSHAN.  \n",
    "Important parameters include:\n",
    "\n",
    "- **`mode`**: GUI/no-GUI mode. For training, this is normally `NoGUI_RL` since it is much faster.  \n",
    "- **`seed`**: Controls reproducibility.  \n",
    "- **`rl_mode`**: Whether the system runs in *training* or *evaluation* mode.  \n",
    "- **`hierarchy_type`**: Selects which agent hierarchy to train or evaluate (`fly_agent`, `planner_agent`).  \n",
    "- **`resume`**: Whether to continue from an existing checkpoint.  \n",
    "- **`auto_train`**: Usually **disabled** in this notebook, but some of its parameters  \n",
    "  - **`max_train`** and **`max_eval`** can still be used when running in NoGUI mode to\n",
    "  determine how long training or evaluation should run before stopping automatically.\n",
    "- **`optuna`**: Hyperparameter search; not relevant for manual training here.\n",
    "\n",
    "These settings determine the *high-level control flow* of ROSHAN.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ **2. `paths` â€” Where Models are Stored and which Map is loaded**\n",
    "This section defines how ROSHAN handles filesystem paths:\n",
    "\n",
    "- **`model_directory`**: Where trained models will be saved or loaded from.  \n",
    "- **`model_name`**: Which checkpoint to load (e.g., `\"latest\"`, `\"best_reward\"`).  \n",
    "- **`init_map`**: Which terrain map is used for simulations in no-GUI mode.  \n",
    "\n",
    "These values allow ROSHAN to store and retrieve models consistently.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  **3. `algorithm` â€” Reinforcement Learning Settings**\n",
    "ROSHAN supports several RL algorithms, but in this notebook we focus on **PPO**.  \n",
    "Important PPO-related parameters include:\n",
    "\n",
    "- **`share_encoder`**: Whether policy and value share a feature extractor.  \n",
    "- **`use_tanh_dist`**: Whether actions use a TanhNormal distribution.  \n",
    "- **`PPO` subsection**:\n",
    "  - **`lr`**: Learning rate.  \n",
    "  - **`batch_size`**, **`horizon`**: How much data is collected before updating.  \n",
    "  - **`k_epochs`**: Number of update passes per batch.  \n",
    "  - **`entropy_coeff`**: Controls exploration.  \n",
    "  - **`value_loss_coef`**: Balances value vs. policy learning.  \n",
    "  - **`gamma`**, **`_lambda`**: Discount and GAE parameters.  \n",
    "  - **`eps_clip`**: PPO clipping range.  \n",
    "  - Optional features: KL early stopping, manual log-std decay, separate optimizers.\n",
    "\n",
    "These parameters define how PPO updates policies for both FlyAgent and PlannerAgent.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ **4. `fire_model` â€” C++ Simulation Parameters**\n",
    "This section configures the wildfire simulator.  \n",
    "It includes cell properties, particle emission, wind conditions, and rendering.\n",
    "\n",
    "For this notebook, **you normally do not modify these values**.  \n",
    "They are used internally by the C++ environment and remain stable for training.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ² **5. `environment` â€” Fire Initialization and Agent Behavior**\n",
    "This section defines how simulations start and how agents are placed:\n",
    "\n",
    "- Initial fire distribution (`num_fire_clusters`, `fire_percentage`)  \n",
    "- Agent behavior (`fire_goal_percentage`, starting positions)  \n",
    "- Evaluation-only features (water limits, recharge time)\n",
    "\n",
    "Again, these are rarely modified from the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš **6. `environment.agent.fly_agent` â€” Low-Level Flight Controller**\n",
    "This section configures the **FlyAgent**, which directly navigates and extinguishes fires.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    "- **`collision`**: Whether the agent must avoid others.  \n",
    "- **`use_simple_policy`**: Episode ends when reaching a goal or when fires are extinguished.  \n",
    "- **`use_velocity_change` / `use_vel_bins`**: Defines the action representation.  \n",
    "- **`frame_skips`**: How often actions are chosen.  \n",
    "- **`algorithm`**: Set to `\"PPO\"` for learning.  \n",
    "- **`num_agents`**: Number of parallel FlyAgents.  \n",
    "- **`time_steps`**: Memory stack length for inputs.  \n",
    "- **`max_speed`, `view_range`**: Movement and observation limits.  \n",
    "- **Rewards**: Shaping terms like distance improvement, boundary penalty, extinguishing bonus, etc.\n",
    "\n",
    "These parameters strongly influence FlyAgent learning behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§­ **7. `environment.agent.planner_agent` â€” High-Level Goal Allocation**\n",
    "The PlannerAgent assigns extinguishing goals to low-level agents:\n",
    "\n",
    "- **`hierarchy_timesteps`**: How many FlyAgent steps per planner step.  \n",
    "- **`algorithm`**: Also uses `\"PPO\"`.  \n",
    "- **`num_agents`**: Number of high-level planners.  \n",
    "- **`view_range`, `max_speed`**: Passed to the internal low-level agents.  \n",
    "- **`default_model_folder` & `default_model_name`**: Which FlyAgent model is used as the plannerâ€™s low-level controller.  \n",
    "- **Rewards**: Focus on map-wide fire suppression, coordination penalties, and prioritization of meaningful goal selection.\n",
    "\n",
    "The PlannerAgent is trained on a much higher abstraction level than the FlyAgent.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will primarily modify:\n",
    "\n",
    "- elements inside **`settings`**  \n",
    "- important values in **`algorithm -> PPO`**  \n",
    "- selected parameters inside **FlyAgent** or **PlannerAgent**\n",
    "\n",
    "Everything elseâ€”especially simulation physicsâ€”is handled automatically by the system. \n",
    "## Now we take a closer look into the two hierarchy settings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb3e2c-5a05-4ed1-8eb3-194ac6978f2b",
   "metadata": {},
   "source": [
    "### ðŸš Configuring the `fly_agent`\n",
    "\n",
    "The `fly_agent` represents ROSHANâ€™s **low-level controller**.  \n",
    "It directly navigates the environment, avoids obstacles, and extinguishes fires.  \n",
    "Below is a brief overview of the key parameters you may want to adjust.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ›‘ **`collision` â€” Collision-aware vs. collision-free training**\n",
    "```python\n",
    "config[\"environment\"][\"agent\"][\"fly_agent\"][\"collision\"] = True\n",
    "```\n",
    "- When enabled, the FlyAgent must avoid other agents.\n",
    "- This changes the network input space and terminal conditions.\n",
    "#### ðŸŽ¯ **`use_simple_policy` â€” Simple vs. complex episode structure**\n",
    "```python\n",
    "config[\"environment\"][\"agent\"][\"fly_agent\"][\"use_simple_policy\"] = False\n",
    "```\n",
    "- **Simple policy**: Episode ends once a single agent reaches its goal.\n",
    "- **Complex policy**: Episode ends when all fires are extinguished(best practice for collision tasks).\n",
    "#### ðŸƒ **Action representation (`use_velocity_change`, `use_vel_bins`)**\n",
    "```python\n",
    "config[\"environment\"][\"agent\"][\"fly_agent\"][\"use_velocity_change\"] = False\n",
    "config[\"environment\"][\"agent\"][\"fly_agent\"][\"use_vel_bins\"] = True\n",
    "```\n",
    "These options determine how actions are interpreted:\n",
    "- use_velocity_change = True: Actions represent accelerations.\n",
    "- use_velocity_change = False: Actions represent absolute velocities.\n",
    "- use_vel_bins = True: Velocities are discretized (100 bins per dimension),making training more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558735e9-31b6-4e83-a1d0-9c6186c15826",
   "metadata": {},
   "source": [
    "### ðŸ§­ Configuring the `planner_agent`\n",
    "\n",
    "The `planner_agent` represents ROSHANâ€™s **high-level decision maker**. Unlike the FlyAgentâ€”which handles local movement and extinguishingâ€”the Planner assigns **goals** to a group of low-level agents and operates on a slower timescale.\n",
    "\n",
    "Below is a brief explanation of the key configuration parameters you may want to adjust when training or evaluating the Planner.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”„ **`hierarchy_timesteps` â€” How often the Planner acts**\n",
    "```python\n",
    "config[\"environment\"][\"agent\"][\"planner_agent\"][\"hierarchy_timesteps\"] = 3\n",
    "```\n",
    "This parameter determines how many low-level FlyAgent steps occur for every one Planner step.\n",
    "\n",
    "- A higher value means the Planner acts less frequently, giving FlyAgents more time to execute each assigned goal.\n",
    "- A lower value means the Planner makes decisions more frequently, leading to finer but potentially noisier coordination.\n",
    "\n",
    "This value strongly influences how â€œstrategicâ€ or â€œreactiveâ€ the Planner becomes.\n",
    "\n",
    "#### ðŸ¤– **Low-level agent parameters (`num_agents, time_steps, max_speed, view_range`)**\n",
    "\n",
    "These values configure the simulated FlyAgents that operate under the Planner during training.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "config[\"environment\"][\"agent\"][\"planner_agent\"][\"num_agents\"] = 4\n",
    "config[\"environment\"][\"agent\"][\"planner_agent\"][\"max_speed\"] = 10.0\n",
    "```\n",
    "\n",
    "#### ðŸ“¦ **`default_model_folder` â€” The FlyAgent model used by the Planner**\n",
    "```python\n",
    "config[\"environment\"][\"agent\"][\"planner_agent\"][\"default_model_folder\"] = \"models/SimpleFlyAgent/\"\n",
    "config[\"environment\"][\"agent\"][\"planner_agent\"][\"default_model_name\"] = \"best_reward\"\n",
    "```\n",
    "\n",
    "This is an important Planner setting. It specifies which FlyAgent model is used as the Plannerâ€™s low-level controller during training and evaluation.\n",
    "\n",
    "- During **training**, you can choose a *simple* FlyAgent (e.g., no collision avoidance, small network), which makes training faster.\n",
    "- During **evaluation**, you may switch this to a more advanced FlyAgent (e.g., with collision avoidance enabled, different action structure).\n",
    "\n",
    "This makes the Planner flexible, it learns the strategy while delegating movement details to whichever FlyAgent model you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab0c9b-44a1-4fd4-aed6-21a1cc2a69bf",
   "metadata": {},
   "source": [
    "### ðŸ›  Modifying the Configuration from Within the Notebook\n",
    "\n",
    "The `config` object is just a nested Python dictionary.  \n",
    "You can change values directly in the notebook before writing out the execution config.\n",
    "\n",
    "Below are some common examples.\n",
    "\n",
    "---\n",
    "\n",
    "#### Change the config to your likings\n",
    "\n",
    "```python\n",
    "# Use NoGUI RL mode\n",
    "config[\"settings\"][\"mode\"] = 0  # 0 = GUI_RL # 2 = NoGUI_RL # 1/4 C++ Sim only\n",
    "\n",
    "# Set the directory where models will be saved/loaded\n",
    "config[\"paths\"][\"model_directory\"] = \"models/NotebookDemoRun\"\n",
    "\n",
    "# (Optional) Set a specific model to load.\n",
    "# Leave as \"\" to let ROSHAN choose automatically (e.g. latest or best).\n",
    "config[\"paths\"][\"model_name\"] = \"\"\n",
    "\n",
    "# Train a model\n",
    "config[\"settings\"][\"rl_mode\"] = \"train\"\n",
    "# OR: Evaluate an existing model\n",
    "config[\"settings\"][\"rl_mode\"] = \"eval\"\n",
    "\n",
    "# Use the low-level FlyAgent controller\n",
    "config[\"settings\"][\"hierarchy_type\"] = \"fly_agent\"\n",
    "# OR: Use the high-level PlannerAgent\n",
    "config[\"settings\"][\"hierarchy_type\"] = \"planner_agent\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca361a41-6699-4b94-800e-4db2e121cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NoGUI RL mode\n",
    "config[\"settings\"][\"mode\"] = 2  # 2 = NoGUI_RL # 0 = GUI_RL\n",
    "\n",
    "# Train the FlyAgent\n",
    "config[\"settings\"][\"rl_mode\"] = \"train\" # 'train' or 'eval'\n",
    "config[\"settings\"][\"hierarchy_type\"] = \"fly_agent\"\n",
    "\n",
    "# Set model output directory\n",
    "config[\"paths\"][\"model_directory\"] = \"models/NotebookDemoRun\"\n",
    "config[\"paths\"][\"model_name\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8cc70-8f41-424b-8c94-741dae846dbf",
   "metadata": {},
   "source": [
    "### â–¶ï¸ Executing ROSHAN with the Modified Configuration\n",
    "\n",
    "Once your changes to the `config` dictionary are complete, the notebook writes the updated configuration to a separate file. This ensures that both Python and the underlying C++ simulator use the exact same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010eb0f1-23d2-473f-a0bd-4e03424864b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config file: /home/nex/Dokumente/Code/ROSHAN/notebook_config_exec.yaml\n",
      "Mode: NoGUI_RL\n",
      "Loading Firemodel without GUI\n",
      "Using seed -537406765\n",
      "Groundstation Start Percentage: 1\n",
      "Imported all Config Parameters from/home/nex/Dokumente/Code/ROSHAN/used_config.yaml\n",
      "Created ReinforcementLearning Handler\n",
      "Running in NoGUI_RL mode. Agent always runs.\n",
      "Created FireModel\n",
      "Firemodel loaded\n",
      "\n",
      "> openstreetmap@1.0.0 start\n",
      "> nodemon server.js > server.log 2>&1 & echo $! > server.pid\n",
      "\n",
      "Started OSM-Server with command status: 0\n",
      "Loading map from: /home/nex/Dokumente/Code/ROSHAN/maps/35x36_Woods.tif\n",
      "2025-12-02 15:12:06,131 - INFO - fly_agent - Logging File at /home/nex/Dokumente/Code/ROSHAN/models/NotebookDemoRun/logs/logging.log\n",
      "2025-12-02 15:12:06,131 - INFO - fly_agent - Network sources loaded from /home/nex/Dokumente/Code/ROSHAN/src/pysim/networks/network_fly.py\n",
      "2025-12-02 15:12:06,644 - INFO - fly_agent - Network sources archived at /home/nex/Dokumente/Code/ROSHAN/models/NotebookDemoRun/networks\n",
      "2025-12-02 15:12:06,735 - INFO - fly_agent - Top Level Hierarchy: fly_agent\n",
      "2025-12-02 15:12:06,735 - INFO - fly_agent - fly_agent initialized\n",
      "2025-12-02 15:12:06,735 - INFO - fly_agent - Algorithm: PPO\n",
      "2025-12-02 15:12:06,735 - INFO - fly_agent - 4 Agents control 4 Drones\n",
      "2025-12-02 15:12:06,735 - INFO - fly_agent - Training from scratch\n",
      "2025-12-02 15:12:06,735 - INFO - HierarchyManager - Hierarchy Level: low\n",
      "2025-12-02 15:13:05,683 - INFO - PPO - Saving at Episode 3381/Train Step 0, best Reward: -0.27\n",
      "2025-12-02 15:13:05,687 - INFO - PPO - Saving at Episode 3381/Train Step 0, best Objective 0.00\n"
     ]
    }
   ],
   "source": [
    "notebook_exec_config_path = os.path.join(root_path, \"notebook_config_exec.yaml\")\n",
    "with open(notebook_exec_config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "    \n",
    "!python src/pysim/main.py \"{notebook_exec_config_path}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
