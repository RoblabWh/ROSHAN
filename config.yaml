settings:
  # 0: GUI_RL, 2: NoGUI_RL, # Mode 1 is GUI and Mode 2 is NoGUI (ONLY Simulation)
  mode: 2
  # Random seed for reproducibility (-1 for random seed)
  seed: -1
  # RL_MODE: Either "train" or "eval"
  rl_mode: train
  # Resume training from a checkpoint
  # If set to true, the training will continue from the specified checkpoint
  resume: false
  # LLM support for the environment (currently not supported; was tested in very early prototypes)
  # Will probably break everything if set to true
  llm_support: false
  # Type of hierarchy to use for the agents (fly_agent, explore_agent, planner_agent)
  hierarchy_type: fly_agent
  # Uses this config as default and skips the GUI setup
  skip_gui_init: true
  # Enable PyTorch autograd anomaly detection (useful for debugging; slows down training)
  pytorch_detect_anomaly: false
  # Enable CIA mode (Covert Infiltration Avian) for government use only!
  cia_mode: false
  # Settings for automatic training of multiple runs under the same config
  auto_train:
    # Use or don't use automatic training
    use_auto_train: false
    # Number of total trainings, each containing max_train train_steps
    # automatically set to 1 when use_auto_train is false
    train_episodes: 10 
    # Maximum number of train_steps(NOT policy steps) to perform
    # this setting is also used in case of NoGUI mode to determine when to stop training and switch to evaluation
    # This setting gets overridden in IQL settings and is calculated as the total number of offline_updates + online_updates
    max_train: 200
    # Maximum number of evaluations(one evaluation = one environment completed) to perform
    max_eval: 200
  optuna:
    use_optuna: false # Whether to use Optuna for hyperparameter optimization
    study_name: "fly_model_study" # Name of the Optuna study
    study_root: "models" # Root directory to store the Optuna study and trials
    n_trials: 300 # Number of trials to perform
    n_startup_trials: 15 # Number of initial random trials before using the optimization algorithm
    n_warmup_steps: 30 # Number of warmup steps before starting the optimization
    use_pruning: false # Prune training episodes with MedianPruner
    objective: "objective" # Objective to optimize ("reward" or "objective")
   # Whether to save the replay buffer when it's full (to model_directory/memory.pkl) 
   # This has only effect in evaluation mode
  save_replay_buffer: false
  # Size of the replay buffer to be saved when save_replay_buffer is true
  save_size: 2.0e+5
paths:
  # This is the root_path where your NEW trained models data is stored
  # Loading for your ROOT Model(chosen hierarchy) is ALSO done from here (Eval and Resuming Training)
  # Absolute or relative to the project root
  model_directory: models/PPOTanhDistri_2
  # Leave this empty to get a default name from this config (best practice for training)
  # You can fast load models with suffixes ["latest", "best_reward", "best_obj"]
  # If you want to load a specific model, set the model_name to the name of the model file
  model_name: ""
  # Name of the CORINE dataset-file, Path to the Corine dataset file must be set in project_paths.json (generated default paths at compile time)
  corine_dataset_name: CLMS_CLCplus_RASTER_2021_010m_eu_03035_V1_1.tif
  # This map is used in NoGUI & skip_gui_init setup, if left empty("") the default map (uniform map) will be used.
  init_map: 35x36_Woods.tif

algorithm:
  # Share the encoder(Inputspace in the network files) between the policy and value networks
  # This option should generally only be used for PPO, but can be used for other algorithms as well
  share_encoder: true
  # Use TanhNormal distribution for stochastic policy networks
  # If set to false, a normal distribution will be used and the actions will be clipped to the action space limits
  # Note: This leads to log_prob issues and suboptimal policies
  # TanhNormal is generally preferred as it naturally bounds the actions within the action space limits
  use_tanh_dist: true
  PPO:
    # Size of the Replay Buffer 
    # Gets cleaned up after each training episode
    # Needs to be bigger when trying to safe the buffer
    memory_size: 1.0e+6
    # Learning rate for the optimizer
    lr: 3.26e-05 # Learning rate for the optimizer
    # Batch size for the optimizer
    batch_size: 1024 #256 
    # Number of steps to collect before updating the model
    horizon: 12800 #10240 #12800 
    # Number of epochs to update the model
    k_epochs: 19 #11 #8 
    # Coefficient for the entropy loss
    entropy_coeff: 0.01 #0.0006
    # Coefficient for the value loss
    value_loss_coef: 0.5
    # Use separate optimizers for the policy and value networks
    # If this is set to false, a single optimizer will be used for 
    # both networks and the value_loss_coef has no effect
    separate_optimizers: false 
    # Beta1 for the Adam optimizer
    beta1: 0.9 
    # Beta2 for the Adam optimizer
    beta2: 0.999 
    # Discount factor for the rewards
    gamma: 0.9635
    # Lambda for the Generalized Advantage Estimation (GAE)
    _lambda: 0.9
    # Clipping value for the PPO loss
    eps_clip: 0.2783
    # Manual decay of the log_std parameter in stochastic policies
    # If set to false, the log_std is learned by the network(state independent)
    manual_decay: false
    # Two types of manual decay implemented: train_step decay and logarithmic decay
    # train_step decay: reduces log_std based on the training step count using an exponential, causing a smooth nonlinear decay over time
    # logarithmic decay: applies the decay to the current log_std, producing a multiplicative, logarithmic reduction per update step
    # If you plan to use manual decay in combination with resuming training logarithmic decay is preferred
    use_logstep_decay: true
    decay_rate: 0.985
  IQL:
    # Path to the replay buffer file (absolute or relative to the project root)
    # Size of the Replay Buffer is determined by the size used to create this buffer
    # Keep this in mind for Online Fine-Tuning
    buffer_path: "models/PPOTanhDistri/memory.pkl"
    # Learning rate for the optimizer
    lr: 1.0e-4
    # Batch size for the optimizer
    batch_size: 256
    # Discount factor for the rewards
    discount: 0.99
    # Expectile for the IQL loss
    expectile: 0.7
    # Target network soft update rate
    tau: 0.005
    # Temperature for the IQL loss
    temperature: 3.0 
    # Frequency of policy updates in online finetuning
    # i.e. how many environment steps between the next policy update
    policy_freq: 2000
    # Offline Training Steps (before eventually doing an online finetuning)
    # One offline update goes through the entire dataset once
    offline_updates: 10
    # Online Training Steps after offline training
    online_updates: 100
    # Number of epochs in online-finetuning, has no effect on offline training
    k_epochs: 1
    # Beta1 for the Adam optimizer
    beta1: 0.9 
    # Beta2 for the Adam optimizer
    beta2: 0.999 
  TD3:
    # Learning rate for the optimizer
    lr: 3.0e-4
    # Batch size for the optimizer
    batch_size: 512
    memory_size: 1.0e+6 # Max Size of the Replay Buffer
    min_memory_size: 100000 # Minimum number of samples in the replay buffer before training starts
    k_epochs: 1 # Number of epochs to update the model
    discount: 0.99 # Discount factor for the rewards
    tau: 0.005 # Target network update rate
    exploration_noise: 0.1 # Exploration noise for the action space
    policy_noise: 0.2 # Noise added to the policy network
    noise_clip: 0.5 # Clipping value for the noise
    policy_freq: 2 # Frequency of policy updates
    beta1: 0.9 # Beta1 for the Adam optimizer
    beta2: 0.999 # Beta2 for the Adam optimizer

fire_model:
  # Rendering Parameters
  rendering:
    render_grid: false
    has_noise: true
    lingering: false
    noise_level: 20
    noise_size: 2
    background_color: [21, 19, 51, 255] #[41, 49, 51, 255]
  #### Simulation Parameters
  # These parameters are used to configure the simulation environment
  # They contain time, cell, particles, grid and wind parameters
  # The values are set to reasonable defaults for a simulation
  # Most of them can be adjusted in the GUI, adjustment here only for testing purposes
  simulation:
    time:
      dt: 0.3
      min_dt: 0.0001
      max_dt: 5.0
    cells:
      # Besides flood_duration these values only affect the uniform grid, otherwise those values are derived from the cell_class
      cell_ignition_threshold: 100.0 # Time that the cell needs to be visited by a particle to be ignited in seconds (s)
      cell_burning_duration: 120.0 # Time the cell is capable of burning in seconds (s)
      cell_size: 10.0 # We assume quadratic cells and this is the length of both sides of the cell in meters (m)
      flood_duration: 5.0 # Flooding duration of a cell in seconds (s)
    particles:
      # These parameters are used to configure the particles in the simulation
      # Ranges are provided in the comments; stray apart from them, and you might get funny results
      emit_convective: true # Whether to emit convective particles
      virtualparticle_y_st: 1.0 # Hotness of the Particle (no real world unit yet) [0.0 - 1.0]
      virtualparticle_y_lim: 0.2 # How long the Particle is able to cause ignition (no real world unit yet) [0.1 - 0.3]
      virtualparticle_fl: 0.15 # Scaling factor for new position (need to calibrate) [0.0 - 10.0]
      virtualparticle_c0: 1.98 # A constant close to 2 [1.5 - 2.0]
      virtualparticle_tau_mem: 10.0 # A few tens of seconds [0.1 - 100.0]
      Lt: 80.0 # Height of emitting source (m)
      emit_radiation: true # Whether to emit radiation particles
      radiationparticle_y_st: 1.0 # Hotness of the Radiation Particle (no real world unit yet) [0.0 - 1.0]
      radiationparticle_y_lim: 0.165 # How long the Radiation Particle is able to cause ignition (no real world unit yet) [0.1 - 0.3]
    grid:
      uniform_nx: 50 # Number of cells in x-direction for uniform grid (Rows programmatically; Cols visually)
      uniform_ny: 50 # Number of cells in y-direction for uniform grid (Cols programmatically; Rows visually)
      exploration_map_size: 50 # Size of the exploration map side (side X side)
    wind:
      wind_uw: 10.0 # Wind Speed at 10m height in m/s [0 - 35(Hurricane)]
      wind_angle: -1.0 # Wind Angle in degree, -1 for random start wind angle
      wind_a: 0.314 # Component of the wind speed in the 1st direction [0.2 - 0.5]

# Environment Parameters
environment:
  # These parameters are used to configure the fire START behaviour in the environment
  # It does NOT affect the actual fire spread during simulation, this is handled by the fire_model
  fire_behaviour:
    # Percentage of cells that should be ignited at the start of the simulation [0.0 - 1.0]
    # Setting this to < 0.0 will lead to randomly generated fire_percentage between (0, 1]
    fire_percentage: 0.01
    # How many clusters of fire should be started at the beginning of the simulation
    # Setting this to <= 0 will lead to single cells being ignited randomly according to fire_percentage
    num_fire_clusters: 1
    # This probability is used to determine if a fire spreads to a neighboring cell
    # It is only used to control starting Fire Clusters, not the actual fire spread during simulation
    # Setting this to < 0.0 will lead to random fire_noise between 0 and 1
    fire_spread_prob: 0.5
    # Noise ([-0.5, 0,5]*noise) for the fire spread probability, to introduce randomness in the fire spread
    # Setting this to < 0.0 will lead to random fire_noise between 0 and 1
    fire_noise: 1
    # TODO: OLD PARAMETER, REMOVE LATER IF NOT NEEDED ANYMORE
    recharge_time_active: false
  agent_behaviour:
    # Percentage of initial goal set (for fly_agents) that are at a fire_position
    fire_goal_percentage: 1
    # Percentage of initial starting positions that are at the groundstation
    groundstation_start_percentage: 1
    # Other parameters that are currently hard-coded or only accessible via GUI will be added here later
  agent:
    # Number of water units a drone can carry (one unit extinguishes one cell)
    # Currently not used, needs to be implemented in the fire extinguishing logic
    water_capacity: 10 
    # Size of the drone in m (used for rendering and collision detection); 
    # for convenience, all drones have the same size
    drone_size: 2 
    fly_agent:
      # Two types of policies can be learned:
      # Simple Policy: The episode ends when one agent reaches its goal_position
      # Complex Policy: The episode ends when all fires are extinguished (this influences the reward structure slightly)
      use_simple_policy: false
      # Two types of action space use:)
      # If true, the agent's action represents a change in velocity (acceleration)
      # If false, the agent's action represents an absolute velocity of the agent's max_velocity
      use_velocity_change: true
      # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      frame_skips: 1 
      # Algorithm used by the fly_agent
      algorithm: PPO
      # Number of fly_agents
      num_agents: 5
      # Number of time steps to consider in the FlyAgent's memory
      time_steps: 3 
      # Maximum speed of the FlyAgent in m/s
      max_speed: 10.0 
      # View range of the FlyAgent in cells
      view_range: 10 
      # Default model path for the fly_agent (either absolute or relative to the project root)
      default_model_folder: models/fly_agent/training_1 
      # You can fast load models with suffixes ["latest", "best_reward", "best_obj"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: latest
    explore_agent:
      # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      frame_skips: 1 
      # Algorithm used by the explore_agent (currently no training implemented, so set to no_algo)
      algorithm: no_algo 
      # Number of explore_agents
      num_agents: 4 
      # Number of time steps to consider in the ExploreAgent's memory
      time_steps: 2 
      # Maximum speed of the explore_agent in m/s
      # Should be set to the max_speed of the fly_agent uses
      max_speed: 10.0 
      # View range of the explore_agent in cells
      view_range: 12 
      # Keep empty for future use and compatibility with pipeline
      default_model_folder: "" 
      # Keep empty for future use and compatibility with pipeline lol
      default_model_name: ""  
    planner_agent:
      # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      frame_skips: 1 
      # Algorithm used by the planner_agent
      algorithm: PPO 
      # These parameters are used to configure the planner_agents LOW-LEVEL AGENTS
      num_agents: 3
      # Number of time steps to consider in the ExtinguishAgent's memory
      time_steps: 3 
      # Maximum speed of the ExtinguishAgent (low-level-agent of the Planner) in m/s
      max_speed: 3.0 
      # View range of the ExtinguishAgent in cells
      view_range: 6 
      # Default model path for the fly_agent (either absolute or relative to the project root)
      default_model_folder: models/planner_kind_of_works/ 
      # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: latest
