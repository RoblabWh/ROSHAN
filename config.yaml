settings:
  # 0: GUI_RL, 2: NoGUI_RL, # Mode 1 is GUI and Mode 2 is NoGUI (ONLY Simulation)
  mode: 2
  # Random seed for reproducibility (-1 for random seed)
  seed: -1
  # RL_MODE: Either "train" or "eval"
  rl_mode: train
  # This parameter determines whether to evaluate the fly_agent policy with the extinguishing heuristic
  eval_fly_policy: false
  # Resume training from a checkpoint
  # If set to true, the training will continue from the specified checkpoint
  resume: false
  # Whether to log evaluation results in tensorboard, plot their metrics and create the CSV files
  # Should generally be true during training or evaluation. 
  # If you only want to SEE what the agent does, set to false so your evaluation is not overridden
  log_eval: true
  # LLM support for the environment (currently not supported; was tested in very early prototypes)
  # Will probably break everything if set to true
  llm_support: false
  # Type of hierarchy to use for the agents (fly_agent, explore_agent, planner_agent)
  hierarchy_type: fly_agent
  # Uses this config as default and skips the GUI setup
  skip_gui_init: true
  # Enable PyTorch autograd anomaly detection (useful for debugging; slows down training)
  pytorch_detect_anomaly: false
  # Enable CIA mode (Covert Infiltration Avian) for government use only!
  cia_mode: false
  # Settings for automatic training of multiple runs under the same config
  auto_train:
    # Use or don't use automatic training
    use_auto_train: false
    # Number of total trainings, each containing max_train train_steps
    # automatically set to 1 when use_auto_train is false
    train_episodes: 10
    # Maximum number of train_steps(NOT policy steps) to perform
    # this setting is also used in case of NoGUI mode to determine when to stop training and switch to evaluation
    # This setting gets overridden in IQL settings and is calculated as the total number of offline_updates + online_updates
    max_train: 400
    # Maximum number of evaluations(one evaluation = one environment completed) to perform
    max_eval: 200
  optuna:
    use_optuna: false # Whether to use Optuna for hyperparameter optimization
    study_name: "fly_model_study" # Name of the Optuna study
    study_root: "models" # Root directory to store the Optuna study and trials
    n_trials: 300 # Number of trials to perform
    n_startup_trials: 15 # Number of initial random trials before using the optimization algorithm
    n_warmup_steps: 30 # Number of warmup steps before starting the optimization
    use_pruning: false # Prune training episodes with MedianPruner
    objective: "objective" # Objective to optimize ("reward", "objective" or "time_to_end")
   # Whether to save the replay buffer when it's full (to model_directory/memory.pkl) 
   # This has only effect in evaluation mode
  save_replay_buffer: false
  # Size of the replay buffer to be saved when save_replay_buffer is true
  save_size: 2.0e+5
paths:
  # This is the root_path where your NEW trained models data is stored
  # Loading for your ROOT Model(chosen hierarchy) is ALSO done from here (Eval and Resuming Training)
  # Absolute or relative to the project root
  model_directory: models/MediumBaseline2 #PPO_4A_NoManualDecay_P9Like
  # Leave this empty to get a default name from this config (best practice for training)
  # You can fast load models with suffixes ["latest", "best_reward", "best_obj"]
  # If you want to load a specific model, set the model_name to the name of the model file
  model_name: "best_obj"
  # Name of the CORINE dataset-file, Path to the Corine dataset file must be set in project_paths.json (generated default paths at compile time)
  corine_dataset_name: CLMS_CLCplus_RASTER_2021_010m_eu_03035_V1_1.tif
  # This map is used in NoGUI & skip_gui_init setup, if left empty("") the default map (uniform map) will be used.
  init_map: 81x129_Campsite.tif #35x36_Woods.tif
#{'entropy_coeff': 1.230811218699769e-06, 'gamma': 0.9773972579784836, '_lambda': 0.9096741778233516, 'eps_clip': 0.22816431818213967}
algorithm:
  # Share the encoder(Inputspace in the network files) between the policy and value networks
  # This option should generally only be used for PPO, but can be used for other algorithms as well
  share_encoder: true
  # Use TanhNormal distribution for stochastic policy networks
  # If set to false, a normal distribution will be used and the actions will be clipped to the action space limits
  # Note: This leads to log_prob issues and suboptimal policies
  # TanhNormal is generally preferred as it naturally bounds the actions within the action space limits
  use_tanh_dist: true
  PPO:
    # Size of the Replay Buffer 
    # Gets cleaned up after each training episode
    # Needs to be bigger when trying to safe the buffer
    memory_size: 1.0e+6
    # Learning rate for the optimizer #1.575e-4 
    lr: 1.575e-4 # Learning rate for the optimizer
    # Batch size for the optimizer
    batch_size: 1000 #2000 #300 #2048
    # Number of steps to collect before updating the model
    horizon: 12000 #66000 #3600 #65536
    # Number of epochs to update the model
    k_epochs: 3
    # Coefficient for the entropy loss
    entropy_coeff: 0.01
    # Coefficient for the value loss (no effect if separate_optimizers is true)
    value_loss_coef: 0.5
    # Use separate optimizers for the policy and value networks
    # If this is set to false, a single optimizer will be used for 
    # both networks (only works if share_encoder is false)
    separate_optimizers: false
    # Beta1 for the Adam optimizer
    beta1: 0.9 
    # Beta2 for the Adam optimizer
    beta2: 0.999 
    # Discount factor for the rewards
    gamma: 0.9635
    # Lambda for the Generalized Advantage Estimation (GAE)
    _lambda: 0.90
    # Clipping value for the PPO loss #0.2783
    eps_clip: 0.2
    # Manual decay of the log_std parameter in stochastic policies
    # If set to false, the log_std is learned by the network(state independent)
    manual_decay: false
    # Two types of manual decay implemented: train_step decay and logarithmic decay
    # train_step decay: reduces log_std based on the training step count using an exponential, causing a smooth nonlinear decay over time
    # logarithmic decay: applies the decay to the current log_std, producing a multiplicative, logarithmic reduction per update step
    # If you plan to use manual decay in combination with resuming training logarithmic decay is preferred
    use_logstep_decay: false
    # ~0.99975 for log_step_decay = true
    # ~0.985 for log_step_decay
    # Both values will lead to gradual decay towards zero over 200 episodes
    decay_rate: 0.9999 #0.99975
    # The approximate KL divergence that is calculated during training and
    # is used to monitor wheather the policy is changing too quickly
    # can be used to implement early stopping based on KL divergence
    # the early stopping will not stop the overall training, but only the current update step
    # If kl_early_stop is true, training will stop early if the kl divergence exceeds kl_target
    kl_early_stop: false
    # This value should ideally be set between 0.01 and 0.05
    kl_target: 0.02
    # Whether to use the first KL divergence estimate or the second (unbiased, lesser variance)
    # The second estimate is less harsh on the cutting, but might lead to larger policy updates
    # kl_1 = (old_logprobs - logprobs).mean()
    # kl_2 = ((ratios - 1) - log_ratio).mean()
    use_kl_1: true
  IQL:
    # Path to the replay buffer file (absolute or relative to the project root)
    # Size of the Replay Buffer is determined by the size used to create this buffer
    # Keep this in mind for Online Fine-Tuning
    buffer_path: "models/MediumBaseline_RewardAltered/memory.pkl"
    # Learning rate for the optimizer
    lr: 1.575e-4
    # Batch size for the optimizer
    batch_size: 1024
    # Discount factor for the rewards
    discount: 0.99
    # Expectile for the IQL loss
    expectile: 0.7
    # Target network soft update rate
    tau: 0.005
    # Temperature for the IQL loss
    temperature: 3.0 
    # Frequency of policy updates in online finetuning
    # i.e. how many environment steps between the next policy update
    policy_freq: 2000
    # Offline Training Steps (before eventually doing an online finetuning)
    # One offline update goes through the entire dataset once
    offline_updates: 50
    # Online Training Steps after offline training
    online_updates: 0
    # Number of epochs in online-finetuning, has no effect on offline training
    k_epochs: 1
    # Beta1 for the Adam optimizer
    beta1: 0.9 
    # Beta2 for the Adam optimizer
    beta2: 0.999 
  TD3:
    # Learning rate for the optimizer
    lr: 3.0e-4
    # Batch size for the optimizer
    batch_size: 512
    memory_size: 1.0e+6 # Max Size of the Replay Buffer
    min_memory_size: 100000 # Minimum number of samples in the replay buffer before training starts
    k_epochs: 1 # Number of epochs to update the model
    discount: 0.99 # Discount factor for the rewards
    tau: 0.005 # Target network update rate
    exploration_noise: 0.1 # Exploration noise for the action space
    policy_noise: 0.2 # Noise added to the policy network
    noise_clip: 0.5 # Clipping value for the noise
    policy_freq: 2 # Frequency of policy updates
    beta1: 0.9 # Beta1 for the Adam optimizer
    beta2: 0.999 # Beta2 for the Adam optimizer

fire_model:
  # Rendering Parameters
  rendering:
    render_grid: false
    has_noise: true
    lingering: false
    noise_level: 20
    noise_size: 2
    background_color: [21, 19, 51, 255] #[41, 49, 51, 255]
  #### Simulation Parameters
  # These parameters are used to configure the simulation environment
  # They contain time, cell, particles, grid and wind parameters
  # The values are set to reasonable defaults for a simulation
  # Most of them can be adjusted in the GUI, adjustment here only for testing purposes
  simulation:
    time:
      dt: 0.1 # 0.3
      min_dt: 0.0001
      max_dt: 5.0
    cells:
      # Besides flood_duration these values only affect the uniform grid, otherwise those values are derived from the cell_class
      cell_ignition_threshold: 100.0 # Time that the cell needs to be visited by a particle to be ignited in seconds (s)
      cell_burning_duration: 120.0 # Time the cell is capable of burning in seconds (s)
      cell_size: 10.0 # We assume quadratic cells and this is the length of both sides of the cell in meters (m)
      flood_duration: 5.0 # Flooding duration of a cell in seconds (s)
    particles:
      # These parameters are used to configure the particles in the simulation
      # Ranges are provided in the comments; stray apart from them, and you might get funny results
      emit_convective: true # Whether to emit convective particles
      virtualparticle_y_st: 1.0 # Hotness of the Particle (no real world unit yet) [0.0 - 1.0]
      virtualparticle_y_lim: 0.2 # How long the Particle is able to cause ignition (no real world unit yet) [0.1 - 0.3]
      virtualparticle_fl: 0.15 # Scaling factor for new position (need to calibrate) [0.0 - 10.0]
      virtualparticle_c0: 1.98 # A constant close to 2 [1.5 - 2.0]
      virtualparticle_tau_mem: 10.0 # A few tens of seconds [0.1 - 100.0]
      Lt: 80.0 # Height of emitting source (m)
      emit_radiation: true # Whether to emit radiation particles
      radiationparticle_y_st: 1.0 # Hotness of the Radiation Particle (no real world unit yet) [0.0 - 1.0]
      radiationparticle_y_lim: 0.165 # How long the Radiation Particle is able to cause ignition (no real world unit yet) [0.1 - 0.3]
    grid:
      uniform_nx: 50 # Number of cells in x-direction for uniform grid (Rows programmatically; Cols visually)
      uniform_ny: 50 # Number of cells in y-direction for uniform grid (Cols programmatically; Rows visually)
      exploration_map_size: 50 # Size of the exploration map side (side X side)
    wind:
      wind_uw: 10.0 # Wind Speed at 10m height in m/s [0 - 35(Hurricane)]
      wind_angle: -1.0 # Wind Angle in degree, -1 for random start wind angle
      wind_a: 0.314 # Component of the wind speed in the 1st direction [0.2 - 0.5]

# Environment Parameters
environment:
  # These parameters are used to configure the fire START behaviour in the environment
  # It does NOT affect the actual fire spread during simulation, this is handled by the fire_model
  fire_behaviour:
    # Percentage of cells that should be ignited at the start of the simulation [0.0 - 1.0]
    # Setting this to < 0.0 will lead to randomly generated fire_percentage between (0, 1]
    fire_percentage: 0.01
    # How many clusters of fire should be started at the beginning of the simulation
    # Setting this to <= 0 will lead to single cells being ignited randomly according to fire_percentage
    num_fire_clusters: 2
    # This probability is used to determine if a fire spreads to a neighboring cell
    # It is only used to control starting Fire Clusters, not the actual fire spread during simulation
    # Setting this to < 0.0 will lead to random fire_noise between 0 and 1
    fire_spread_prob: 0.5
    # Noise ([-0.5, 0,5]*noise) for the fire spread probability, to introduce randomness in the fire spread
    # Setting this to < 0.0 will lead to random fire_noise between 0 and 1
    fire_noise: 1
  agent_behaviour:
    # Percentage of initial goal set (for fly_agents) that are at a fire_position
    fire_goal_percentage: 1
    # Percentage of initial starting positions that are at the groundstation
    groundstation_start_percentage: 1
    # Other parameters that are currently hard-coded or only accessible via GUI will be added here later
  agent:
    # If true, drones have a water limit and need to recharge at the groundstation
    # Should only be used in evaluation mode for fly_policy eval for now
    use_water_limit: false
    # Number of water units a drone can carry (one unit extinguishes one cell)
    water_capacity: 5
    # Time in seconds to recharge water at the groundstation (set this to 0 for instant recharge)
    recharge_time: 0.0
    # Size of the drone in m (used for rendering and collision detection);
    # for convenience, all drones have the same size
    drone_size: 2
    fly_agent:
      # Whether the agent should avoid collisions with other agents
      # This changes the network input space and terminal conditions
      collision: true
      # Two types of policies can be learned:
      # Simple Policy: The episode ends when one agent reaches its goal_position
      # Complex Policy: The episode ends when all fires are extinguished (this influences the reward structure slightly)
      use_simple_policy: false
      # Two types of action space use:
      # If true, the agent's action represents accelerations (half times max_velocity^2)
      # If false, the agent's action represents an absolute velocity of the agent's max_velocity
      use_velocity_change: false
      # Discretizes the velocity into 100 bins per action dimension
      use_vel_bins: true
      # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      frame_skips: 1 
      # Algorithm used by the fly_agent
      algorithm: PPO
      # Number of fly_agents
      num_agents: 4
      # Number of time steps to consider in the FlyAgent's memory
      time_steps: 3
      # Maximum speed of the FlyAgent in m/s
      max_speed: 10.0
      # View range of the FlyAgent in cells
      view_range: 10 
      rewards:
        # Reward for reaching the goal (depending on simple or complex policy)
        GoalReached: 0.22955571088458304 #0.4177111856211469 #1.0
        # Penalty for going out of map bounds
        BoundaryTerminal: -4.73712001364079 #-4.6304964328217935 #-1.0
        # Reward for extinguishing a fire cell (only complex policy)
        Extinguish: 0.012840080439462583 #0.0046422313960206695 #0.01
        # Penalty for each time step to encourage faster completion
        TimeOut: -3.049763143781944 #-4.226137821180779 #-1.0
        # Penalty for colliding with another agent
        Collision: -4.73712001364079 #-4.6304964328217935 #-1.0
        # Reward FACTOR for improving distance to the goal
        DistanceImprovement: 0.2778255915926747 #0.32301751873846973 #0.1
        # Penalty FACTOR for being to close to other agents
        ProximityPenalty: -0.18892218077569323 #-0.09674793059735592 #-0.05
    explore_agent:
      # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      frame_skips: 1 
      # Algorithm used by the explore_agent (currently no training implemented, so set to no_algo)
      algorithm: no_algo 
      # Number of explore_agents
      num_agents: 12
      # Number of time steps to consider in the ExploreAgent's memory
      time_steps: 4
      # Maximum speed of the explore_agent in m/s
      # Should be set to the max_speed of the fly_agent uses
      max_speed: 20.0 
      # View range of the explore_agent in cells
      view_range: 20 
      # Default model path for the fly_agent (either absolute or relative to the project root)
      default_model_folder: "models/SimpleInput/" 
      # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: best_obj
    planner_agent:
      # Hierarchy Timesteps (number of low-level steps per high-level step)
      hierarchy_timesteps: 20
      # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      frame_skips: 1 
      # Algorithm used by the planner_agent
      algorithm: PPO 
      # These parameters are used to configure the planner_agents LOW-LEVEL AGENTS
      num_agents: 4
      # Number of time steps to consider in the ExtinguishAgent's memory
      time_steps: 3
      # Maximum speed of the ExtinguishAgent (low-level-agent of the Planner) in m/s
      max_speed: 10.0 
      # View range of the ExtinguishAgent in cells
      view_range: 10 
      # Default model path for the fly_agent (either absolute or relative to the project root)
      default_model_folder: "models/BigBaseline_RewardAltered/"
      # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: best_obj
      rewards:
        # Reward for reaching the goal (extinguishing all fires)
        GoalReached: 1.0
        # Penalty for the Map being burned too much
        MapBurnedTooMuch: -2.0
        # Penalty for selecting the default goal (needed for the attention network when there are no valid fires)
        FlyingTowardsGroundStation: -0.01
        # Penalty for each time step to encourage faster completion
        TimeOut: -2.0
        # Penalty FACTOR for selecting the same goal for multiple agents
        SameGoalPenalty: -0.1
        # Reward FACTOR for each fire cell extinguished from the low level agents during one high-level step
        ExtinguishFires: 0.2
        # Reward FACTOR for fast extinguishing
        FastExtinguish: 0.5