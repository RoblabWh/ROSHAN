settings:
  mode: 0 # 0: GUI_RL, 2: NoGUI_RL, # Mode 1 is GUI and Mode 2 is NoGUI (ONLY Simulation)
  seed: -1 # Random seed for reproducibility (-1 for random seed)
  rl_mode: train # either "train" or "eval"
  # Whether to resume training from the last checkpoint
  # If set to true, the training will continue from the last checkpoint
  # from the default_model_paths for the hierarchy_type agent
  resume: false
  llm_support: false # Whether to use LLM support for the environment (currently not supported)
  hierarchy_type: fly_agent # Type of hierarchy to use for the agents (fly_agent, explore_agent, planner_agent)
  skip_gui_init: true # Uses this config as default and skips the GUI setup
  pytorch_detect_anomaly: false # Enable PyTorch autograd anomaly detection
  auto_train:
    use_auto_train: true # Whether to use auto training
    train_episodes: 10 # Number of total trainings, each containing max_train train_steps
    max_train: 25 # Maximum number of train_steps(NOT policy steps) to perform
    max_eval: 200 # Maximum number of evaluations(one evaluation = one environment completed) to perform
paths:
  # This is the root_path where your NEW trained models data is stored
  # Loading for your ROOT Model (chosen hierarchy) is ALSO done from here (Eval and Resuming Training)
  # (either absolute or relative to the project root)
  model_directory: models/test
  # Leave this empty to get a default name from this config (best practice for training)
  # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
  # If you want to load a specific model, set the model_name to the name of the model file
  model_name: ""
  # Name of the CORINE dataset-file, Path to the Corine dataset file must be set in project_paths.json (generated default paths at compile time)
  corine_dataset_name: CLMS_CLCplus_RASTER_2021_010m_eu_03035_V1_1.tif
  # This map is used in NoGUI & skip_gui_init setup, if left empty("") the default map (uniform map) will be used.
  # TODO Make this a list of maps to choose from
  init_map: 35x36_Woods.tif

algorithm:
  PPO:
    memory_size: 1.0e+5 # Size of the Replay Buffer
    lr: 4.0e-4 # Learning rate for the optimizer
    batch_size: 256 # Batch size for the optimizer
    horizon: 12800 #12800 # Number of steps to collect before updating the model
    k_epochs: 14 #8 # Number of epochs to update the model
    entropy_coeff: 0.0006 # Coefficient for the entropy loss
    value_loss_coef: 0.5 # Coefficient for the value loss
    separate_optimizers: false # Whether to use separate optimizers for the policy and value networks
    beta1: 0.9 # Beta1 for the Adam optimizer
    beta2: 0.999 # Beta2 for the Adam optimizer
    gamma: 0.9635 # Discount factor for the rewards
    _lambda: 0.96 # Lambda for the Generalized Advantage Estimation (GAE)
    eps_clip: 0.2783 # Clipping value for the PPO loss
  IQL:
    memory_size: 1e6 # Size of the Replay Buffer
    lr: 4.0e-4 # Learning rate for the optimizer
    batch_size: 64 # Batch size for the optimizer
    discount: 0.99 #
    expectile: 0.7 # Expectile for the IQL loss
    tau: 0.005 # Target network update rate
    temperature: 3.0 # Temperature for the IQL loss
    min_memory_size: 1000 # Minimum number of samples in the replay buffer before training starts
    policy_freq: 10 #
    k_epochs: 10 # Number of epochs to update the model
    beta1: 0.9 # Beta1 for the Adam optimizer
    beta2: 0.999 # Beta2 for the Adam optimizer
  TD3:
    memory_size: 1.0e+5 # Size of the Replay Buffer
    min_memory_size: 25000 # Minimum number of samples in the replay buffer before training starts
    k_epochs: 20 # Number of epochs to update the model
    discount: 0.99 # Discount factor for the rewards
    tau: 0.005 # Target network update rate
    exploration_noise: 0.1 # Exploration noise for the action space
    policy_noise: 0.2 # Noise added to the policy network
    noise_clip: 0.5 # Clipping value for the noise
    policy_freq: 2 # Frequency of policy updates
    beta1: 0.9 # Beta1 for the Adam optimizer
    beta2: 0.999 # Beta2 for the Adam optimizer

fire_model:
  # Rendering Parameters
  rendering:
    render_grid: false
    has_noise: true
    lingering: false
    noise_level: 20
    noise_size: 2
    background_color: [21, 19, 51, 255] #[41, 49, 51, 255]
  #### Simulation Parameters
  # These parameters are used to configure the simulation environment
  # They contain time, cell, particles, grid and wind parameters
  # The values are set to reasonable defaults for a simulation
  # Most of them can be adjusted in the GUI, adjustment here only for testing purposes
  simulation:
    time:
      dt: 0.3
      min_dt: 0.0001
      max_dt: 5.0
    cells:
      # Besides flood_duration these values only affect the uniform grid, otherwise those values are derived from the cell_class
      cell_ignition_threshold: 100.0 # Time that the cell needs to be visited by a particle to be ignited in seconds (s)
      cell_burning_duration: 120.0 # Time the cell is capable of burning in seconds (s)
      cell_size: 10.0 # We assume quadratic cells and this is the length of both sides of the cell in meters (m)
      flood_duration: 5.0 # Flooding duration of a cell in seconds (s)
    particles:
      # These parameters are used to configure the particles in the simulation
      # Ranges are provided in the comments; stray apart from them, and you might get funny results
      emit_convective: true # Whether to emit convective particles
      virtualparticle_y_st: 1.0 # Hotness of the Particle (no real world unit yet) [0.0 - 1.0]
      virtualparticle_y_lim: 0.2 # How long the Particle is able to cause ignition (no real world unit yet) [0.1 - 0.3]
      virtualparticle_fl: 0.15 # Scaling factor for new position (need to calibrate) [0.0 - 10.0]
      virtualparticle_c0: 1.98 # A constant close to 2 [1.5 - 2.0]
      virtualparticle_tau_mem: 10.0 # A few tens of seconds [0.1 - 100.0]
      Lt: 80.0 # Height of emitting source (m)
      emit_radiation: true # Whether to emit radiation particles
      radiationparticle_y_st: 1.0 # Hotness of the Radiation Particle (no real world unit yet) [0.0 - 1.0]
      radiationparticle_y_lim: 0.165 # How long the Radiation Particle is able to cause ignition (no real world unit yet) [0.1 - 0.3]
    grid:
      uniform_nx: 50 # Number of cells in x-direction for uniform grid (Rows programmatically; Cols visually)
      uniform_ny: 50 # Number of cells in y-direction for uniform grid (Cols programmatically; Rows visually)
      exploration_map_size: 50 # Size of the exploration map side (side X side)
    wind:
      wind_uw: 10.0 # Wind Speed at 10m height in m/s [0 - 35(Hurricane)]
      wind_angle: -1.0 # Wind Angle in degree, -1 for random start wind angle
      wind_a: 0.314 # Component of the wind speed in the 1st direction [0.2 - 0.5]

# Environment Parameters
environment:
  # These parameters are used to configure the fire START behaviour in the environment
  # It does NOT affect the actual fire spread during simulation
  fire_behaviour:
    # Percentage of cells that should be ignited at the start of the simulation [0.0 - 1.0]
    # Setting this to < 0.0 will lead to randomly generated fire_percentage between (0, 1]
    fire_percentage: 0.01
    # How many clusters of fire should be started at the beginning of the simulation
    # Setting this to <= 0 will lead to single cells being ignited randomly according to fire_percentage
    num_fire_clusters: 1
    # This probability is used to determine if a fire spreads to a neighboring cell
    # It is only used to control starting Fire Clusters, not the actual fire spread during simulation
    # Setting this to < 0.0 will lead to random fire_noise between 0 and 1
    fire_spread_prob: 0.5
    # Noise ([-0.5, 0,5]*noise) for the fire spread probability, to introduce randomness in the fire spread
    # Setting this to < 0.0 will lead to random fire_noise between 0 and 1
    fire_noise: 1
    recharge_time_active: false
  agent_behaviour:
    fire_goal_percentage: 0.1
    groundstation_start_percentage: 0.9
  agent:
    water_capacity: 10 # Number of water units a drone can carry (one unit extinguishes one cell)
    max_velocity: 10.0 # Maximum Velocity for a drone agent in m/s
    extinguish_all_fires: false # Whether a fly_agent should extinguish all fires in Evaluation Mode (not really relevant)
    drone_size: 2 # Size of the drone in m (used for rendering and collision detection); for convenience, all drones have the same size
    fly_agent:
      frame_skips: 1 # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      algorithm: PPO # Algorithm used by the fly_agent
      num_agents: 5
      time_steps: 3 # Number of time steps to consider in the FlyAgent's memory
      max_speed: 10.0 # Maximum speed of the FlyAgent in m/s
      view_range: 10 # View range of the FlyAgent in cells
      default_model_folder: models/fly_agent/training_1 # Default model path for the fly_agent (either absolute or relative to the project root)
      # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: latest
    explore_agent:
      frame_skips: 1 # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      algorithm: no_algo # Algorithm used by the explore_agent
      num_agents: 4
      time_steps: 2 # Number of time steps to consider in the ExploreAgent's memory
      max_speed: 10.0 # Maximum speed of the explore_agent in m/s
      view_range: 12 # View range of the explore_agent in cells
      default_model_folder: "" # Keep empty for future use and compatibility with pipeline lol
      # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: ""  # Keep empty for future use and compatibility with pipeline lol
    planner_agent:
      frame_skips: 1 # Determines the frequency of action selection (1 = every frame, 2 = every 2nd frame, etc.)
      algorithm: PPO # Algorithm used by the planner_agent
      # These parameters are used to configure the planner_agents LOW-LEVEL AGENTS
      num_agents: 3
      time_steps: 3 # Number of time steps to consider in the ExtinguishAgent's memory
      max_speed: 3.0 # Maximum speed of the ExtinguishAgent (low-level-agent of the Planner) in m/s
      view_range: 6 # View range of the ExtinguishAgent in cells
      default_model_folder: models/planner_kind_of_works/ # Default model path for the fly_agent (either absolute or relative to the project root)
      # You can fast load models with suffixes ["latest", "best_reward", "best_objective"]
      # If you want to load a specific model, set the model_name to the name of the model file
      default_model_name: latest